{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjBpyHIV/RB25STMCFvzBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngoda/Conversations/blob/master/ch7BOOK2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*CHAPTER 7*\n",
        "\n"
      ],
      "metadata": {
        "id": "OS1hc-uiq7xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Implementation of Sentence Generation**"
      ],
      "metadata": {
        "id": "7WJYQKE9GOC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.functions import softmax\n",
        "from rnnlm import Rnnlm\n",
        "from better_rnnlm import BetterRnnlm\n",
        "\n",
        "# Define a class for generating text using RNNLM.\n",
        "class RnnlmGen(Rnnlm):\n",
        "    # Generate text starting with a given word ID.\n",
        "    # Arguments:\n",
        "    # - start_id: The ID of the starting word.\n",
        "    # - skip_ids: A list of word IDs to skip during generation.\n",
        "    # - sample_size: The maximum number of words to generate.\n",
        "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
        "        word_ids = [start_id]  # Initialize the list of generated word IDs.\n",
        "\n",
        "        x = start_id  # Set the starting word ID.\n",
        "        # Generate text until the sample size is reached.\n",
        "        while len(word_ids) < sample_size:\n",
        "            x = np.array(x).reshape(1, 1)  # Reshape the input for prediction.\n",
        "            score = self.predict(x)  # Get the prediction scores.\n",
        "            p = softmax(score.flatten())  # Calculate the softmax probabilities.\n",
        "\n",
        "            # Sample the next word based on the probabilities.\n",
        "            sampled = np.random.choice(len(p), size=1, p=p)\n",
        "            # Check if the sampled word should be skipped.\n",
        "            if (skip_ids is None) or (sampled not in skip_ids):\n",
        "                x = sampled  # Set the sampled word as the next input.\n",
        "                word_ids.append(int(x))  # Append the sampled word ID to the list.\n",
        "\n",
        "        return word_ids  # Return the generated sequence of word IDs.\n",
        "\n",
        "    # Get the current state of the LSTM layer.\n",
        "    def get_state(self):\n",
        "        return self.lstm_layer.h, self.lstm_layer.c\n",
        "\n",
        "    # Set the state of the LSTM layer.\n",
        "    def set_state(self, state):\n",
        "        self.lstm_layer.set_state(*state)\n",
        "\n",
        "# Define a class for generating text using BetterRNNLM.\n",
        "class BetterRnnlmGen(BetterRnnlm):\n",
        "    # Generate text starting with a given word ID.\n",
        "    # Arguments:\n",
        "    # - start_id: The ID of the starting word.\n",
        "    # - skip_ids: A list of word IDs to skip during generation.\n",
        "    # - sample_size: The maximum number of words to generate.\n",
        "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
        "        word_ids = [start_id]  # Initialize the list of generated word IDs.\n",
        "\n",
        "        x = start_id  # Set the starting word ID.\n",
        "        # Generate text until the sample size is reached.\n",
        "        while len(word_ids) < sample_size:\n",
        "            x = np.array(x).reshape(1, 1)  # Reshape the input for prediction.\n",
        "            score = self.predict(x).flatten()  # Get the prediction scores.\n",
        "            p = softmax(score).flatten()  # Calculate the softmax probabilities.\n",
        "\n",
        "            # Sample the next word based on the probabilities.\n",
        "            sampled = np.random.choice(len(p), size=1, p=p)\n",
        "            # Check if the sampled word should be skipped.\n",
        "            if (skip_ids is None) or (sampled not in skip_ids):\n",
        "                x = sampled  # Set the sampled word as the next input.\n",
        "                word_ids.append(int(x))  # Append the sampled word ID to the list.\n",
        "\n",
        "        return word_ids  # Return the generated sequence of word IDs.\n",
        "\n",
        "    # Get the current states of all LSTM layers.\n",
        "    def get_state(self):\n",
        "        states = []  # Initialize the list of LSTM states.\n",
        "        # Iterate over all LSTM layers and append their states.\n",
        "        for layer in self.lstm_layers:\n",
        "            states.append((layer.h, layer.c))\n",
        "        return states  # Return the list of LSTM states.\n",
        "\n",
        "    # Set the states of all LSTM layers.\n",
        "    def set_state(self, states):\n",
        "        # Iterate over LSTM layers and set their states using the provided states.\n",
        "        for layer, state in zip(self.lstm_layers, states):\n",
        "            layer.set_state(*state)\n"
      ],
      "metadata": {
        "id": "OZxr9rRPBhed"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Append parent directory to the system path.\n",
        "sys.path.append('..')\n",
        "\n",
        "from rnnlm_gen import RnnlmGen  # Import RnnlmGen class from rnnlm_gen module.\n",
        "from dataset import ptb  # Import ptb module from dataset package.\n",
        "\n",
        "# Load the Penn Treebank (PTB) dataset.\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)  # Get the size of the vocabulary.\n",
        "corpus_size = len(corpus)  # Get the size of the corpus.\n",
        "\n",
        "# Initialize RnnlmGen model and load pre-trained parameters.\n",
        "model = RnnlmGen()\n",
        "model.load_params('./Rnnlm.pkl')\n",
        "\n",
        "# Set the start word and skip words for text generation.\n",
        "start_word = 'you'  # Define the starting word.\n",
        "start_id = word_to_id[start_word]  # Get the ID of the starting word.\n",
        "skip_words = ['N', '<unk>', '$']  # Define words to skip during generation.\n",
        "skip_ids = [word_to_id[w] for w in skip_words]  # Get IDs of skip words.\n",
        "\n",
        "# Generate text using the RnnlmGen model.\n",
        "word_ids = model.generate(start_id, skip_ids)  # Generate word IDs.\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])  # Convert word IDs to text.\n",
        "txt = txt.replace(' <eos>', '.\\n')  # Replace end-of-sentence tags.\n",
        "print(txt)  # Print the generated text.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-cBHOgQEzJK",
        "outputId": "874972c8-45d5-45dc-c71b-9da957dadbe6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you do their yen many business concerns will immediately be used in interest.\n",
            " a duck scientific and complains george mitchell will soon operate his positions to meet almost cut due to the league 's economy.\n",
            " he noted that the majority of the elderly was always literally too high next season.\n",
            " last march mr. competitiveness insisted on toronto a spokesman on industrial products.\n",
            " in his letter mr. jones is active as usual in august martin who owns.\n",
            " once including evident credentials statistics see and he is like a lawsuits.\n",
            " mr. roman attributed his stake and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from dataset import sequence\n",
        "import numpy\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "  sequence.load_data('addition.txt', seed=1984)\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "print(x_train.shape, t_train.shape)\n",
        "print(x_test.shape, t_test.shape)\n",
        "# (45000, 7) (45000, 5)\n",
        "# (5000, 7) (5000, 5)\n",
        "\n",
        "print(x_train[0])\n",
        "print(t_train[0])\n",
        "# [ 3  0  2  0  0 11  5]\n",
        "# [ 6  0 11  7  5]\n",
        "\n",
        "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
        "print(''.join([id_to_char[c] for c in t_train[0]]))\n",
        "# 71+118\n",
        "# _189"
      ],
      "metadata": {
        "id": "9R3POUf0Loxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Implementation of seq2seq**"
      ],
      "metadata": {
        "id": "sfs5zhcmKW49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encoder Class\n",
        "''''This code defines an Encoder class used in sequence-to-sequence models.\n",
        "The Encoder class contains an embedding layer and an LSTM layer.\n",
        "It handles the forward and backward passes through the network'''\n",
        "\n",
        "import sys\n",
        "\n",
        "# Append parent directory to the system path.\n",
        "sys.path.append('..')\n",
        "\n",
        "from common.time_layers import *  # Import classes from common.time_layers module.\n",
        "from common.base_model import BaseModel  # Import BaseModel class from common.base_model module.\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    # Initialize the Encoder class.\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # Initialize embedding weights, LSTM weights, and LSTM biases.\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "\n",
        "        # Create TimeEmbedding and TimeLSTM layers.\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
        "\n",
        "        # Combine parameters and gradients.\n",
        "        self.params = self.embed.params + self.lstm.params  # List of parameters.\n",
        "        self.grads = self.embed.grads + self.lstm.grads  # List of gradients.\n",
        "        self.hs = None  # Hidden states.\n",
        "\n",
        "    # Forward pass of the encoder.\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        self.hs = hs\n",
        "        return hs[:, -1, :]  # Return the last hidden state.\n",
        "\n",
        "    # Backward pass of the encoder.\n",
        "    def backward(self, dh):\n",
        "        dhs = np.zeros_like(self.hs)\n",
        "        dhs[:, -1, :] = dh\n",
        "\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout\n"
      ],
      "metadata": {
        "id": "CflDi2zOPE9I"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder:\n",
        "    # Initialize the Decoder class.\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # Initialize embedding, LSTM, and affine layer parameters.\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # Create TimeEmbedding, TimeLSTM, and TimeAffine layers.\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        # Combine parameters and gradients.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params  # List of parameters.\n",
        "            self.grads += layer.grads  # List of gradients.\n",
        "\n",
        "    # Forward pass of the decoder.\n",
        "    def forward(self, xs, h):\n",
        "        self.lstm.set_state(h)  # Set the initial hidden state of LSTM.\n",
        "\n",
        "        out = self.embed.forward(xs)  # Forward pass through embedding layer.\n",
        "        out = self.lstm.forward(out)  # Forward pass through LSTM layer.\n",
        "        score = self.affine.forward(out)  # Compute scores using affine layer.\n",
        "        return score  # Return the output scores.\n",
        "\n",
        "    # Backward pass of the decoder.\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)  # Backward pass through affine layer.\n",
        "        dout = self.lstm.backward(dout)  # Backward pass through LSTM layer.\n",
        "        dout = self.embed.backward(dout)  # Backward pass through embedding layer.\n",
        "        dh = self.lstm.dh  # Get the gradient of hidden state.\n",
        "        return dh  # Return the gradient of hidden state.\n",
        "\n",
        "    # Generate sequence using the decoder.\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []  # Initialize the list to store sampled IDs.\n",
        "        sample_id = start_id  # Initialize the starting ID.\n",
        "\n",
        "        self.lstm.set_state(h)  # Set the initial hidden state of LSTM.\n",
        "\n",
        "        # Generate sequence of specified length.\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array(sample_id).reshape((1, 1))  # Reshape the input.\n",
        "            out = self.embed.forward(x)  # Forward pass through embedding layer.\n",
        "            out = self.lstm.forward(out)  # Forward pass through LSTM layer.\n",
        "            score = self.affine.forward(out)  # Compute scores using affine layer.\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())  # Sample the next ID.\n",
        "            sampled.append(int(sample_id))  # Append the sampled ID.\n",
        "\n",
        "        return sampled  # Return the generated sequence.\n"
      ],
      "metadata": {
        "id": "EMbmt0vnPQug"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(BaseModel):\n",
        "    # Initialize the Seq2seq model.\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "\n",
        "        # Initialize encoder and decoder.\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = Decoder(V, D, H)\n",
        "\n",
        "        # Initialize softmax with loss layer.\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        # Combine parameters and gradients.\n",
        "        self.params = self.encoder.params + self.decoder.params  # List of parameters.\n",
        "        self.grads = self.encoder.grads + self.decoder.grads  # List of gradients.\n",
        "\n",
        "    # Forward pass of the Seq2seq model.\n",
        "    def forward(self, xs, ts):\n",
        "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]  # Prepare input and target sequences.\n",
        "\n",
        "        h = self.encoder.forward(xs)  # Forward pass through encoder.\n",
        "        score = self.decoder.forward(decoder_xs, h)  # Forward pass through decoder.\n",
        "        loss = self.softmax.forward(score, decoder_ts)  # Compute loss using softmax with loss.\n",
        "        return loss  # Return the loss.\n",
        "\n",
        "    # Backward pass of the Seq2seq model.\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.softmax.backward(dout)  # Backward pass through softmax with loss.\n",
        "        dh = self.decoder.backward(dout)  # Backward pass through decoder.\n",
        "        dout = self.encoder.backward(dh)  # Backward pass through encoder.\n",
        "        return dout  # Return the gradient.\n",
        "\n",
        "    # Generate sequence using the Seq2seq model.\n",
        "    def generate(self, xs, start_id, sample_size):\n",
        "        h = self.encoder.forward(xs)  # Forward pass through encoder.\n",
        "        sampled = self.decoder.generate(h, start_id, sample_size)  # Generate sequence using decoder.\n",
        "        return sampled  # Return the generated sequence.\n"
      ],
      "metadata": {
        "id": "2qTOwpWOPgZQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules.\n",
        "import sys\n",
        "\n",
        "# Append parent directory to the system path.\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import libraries for data processing, optimization, training, and evaluation.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from seq2seq import Seq2seq\n",
        "from peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "# Load dataset and get vocabulary dictionaries.\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# Reverse input sequences if specified.\n",
        "is_reverse = True\n",
        "if is_reverse:\n",
        "    x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# Set hyperparameters.\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 128\n",
        "batch_size = 128\n",
        "max_epoch = 25\n",
        "max_grad = 5.0\n",
        "\n",
        "# Instantiate Seq2seq or PeekySeq2seq model.\n",
        "model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "peeky_acc_list = []\n",
        "# Train the model for the specified number of epochs.\n",
        "for epoch in range(max_epoch):\n",
        "    # Train the model for one epoch.\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\n",
        "                batch_size=batch_size, max_grad=max_grad, eval_interval=150)\n",
        "\n",
        "    correct_num = 0\n",
        "    # Evaluate the model on the test dataset.\n",
        "    for i in range(len(x_test)):\n",
        "        question, correct = x_test[[i]], t_test[[i]]\n",
        "        verbose = i < 10\n",
        "        correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse)\n",
        "\n",
        "    # Calculate and store the accuracy.\n",
        "    acc = float(correct_num) / len(x_test)\n",
        "    peeky_acc_list.append(acc)\n",
        "    print('검증 정확도 %.3f%%' % (acc * 100))\n",
        "\n",
        "# Plot the accuracy graph.\n",
        "x_peeky = np.arange(len(peeky_acc_list))\n",
        "plt.plot(x_peeky, peeky_acc_list, marker='o')\n",
        "plt.xlabel('에폭')\n",
        "plt.ylabel('정확도')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nwiiIdtsPzfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}