{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngoda/Conversations/blob/master/b2CHAPTER4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2cmibIBknBX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3NLMl2zkqHG",
        "outputId": "780b6aa6-4bb1-4f06-88bb-e6201e8bc1ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2],\n",
              "       [ 3,  4,  5],\n",
              "       [ 6,  7,  8],\n",
              "       [ 9, 10, 11],\n",
              "       [12, 13, 14],\n",
              "       [15, 16, 17],\n",
              "       [18, 19, 20]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Create a NumPy array containing integers from 0 to 20, reshaped into a 7x3 matrix\n",
        "W = np.arange(21).reshape(7, 3)\n",
        "W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILDfF_IqkwMg",
        "outputId": "deab8901-c65d-465c-c20e-1c816bd2cce9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 7, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "W[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8TBjGzilgx3",
        "outputId": "65ca8193-9bbe-4d47-c5c4-5514262bb3b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15, 16, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "W[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-E_HSfhljtX",
        "outputId": "a6b4cdb9-b991-430c-9ce7-e6a2529667e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  4,  5],\n",
              "       [ 0,  1,  2],\n",
              "       [ 9, 10, 11],\n",
              "       [ 0,  1,  2]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "idx = np.array([1, 0, 3, 0])\n",
        "W[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "smYWJxoNlomd"
      },
      "outputs": [],
      "source": [
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        # Constructor method for the Embedding layer\n",
        "        self.params = [W]  # List to store parameters (embedding weights)\n",
        "        self.grads = [np.zeros_like(W)]  # List to store gradients\n",
        "        self.idx = None  # Index variable for tracking input indices\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # Forward pass method for the Embedding layer\n",
        "        W, = self.params  # Unpack the parameters\n",
        "        self.idx = idx  # Save the input indices\n",
        "        out = W[idx]  # Retrieve the embeddings corresponding to input indices\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        # Backward pass method for the Embedding layer\n",
        "        dW, = self.grads  # Unpack the gradients\n",
        "        dW[...] = 0  # Reset gradients\n",
        "        np.add.at(dW, self.idx, dout)  # Accumulate gradients for input indices\n",
        "        return None  # No gradients with respect to the input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4BPDR1iklqDD"
      },
      "outputs": [],
      "source": [
        "def backward(self, dout):\n",
        "    # Backward pass method for the Embedding layer\n",
        "    dW, = self.grads  # Unpack the gradients\n",
        "    dW[...] = 0  # Reset gradients\n",
        "\n",
        "    # Accumulate gradients for input indices\n",
        "    for i, word_id in enumerate(self.idx):\n",
        "        dW[word_id] += dout[i]\n",
        "\n",
        "    return None  # No gradients with respect to the input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Cx07YNl0Oc"
      },
      "source": [
        "**negative_sampling_layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3L0mW6u_l0vw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')  # Adds parent directory to the system path\n",
        "\n",
        "import collections  # Imports the collections module\n",
        "from common.np import *  # Imports NumPy functions from a custom module\n",
        "from common.layers import Embedding, SigmoidWithLoss  # Imports custom layers for neural networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fSJqIZn3l7PM"
      },
      "outputs": [],
      "source": [
        "class EmbeddingDot:\n",
        "    def __init__(self, W):\n",
        "        # Constructor method for EmbeddingDot layer\n",
        "        self.embed = Embedding(W)  # Initialize Embedding layer\n",
        "        self.params = self.embed.params  # Parameters of Embedding layer\n",
        "        self.grads = self.embed.grads  # Gradients of Embedding layer\n",
        "        self.cache = None  # Cache for intermediate data during backward pass\n",
        "\n",
        "    def forward(self, h, idx):\n",
        "        # Forward pass method for EmbeddingDot layer\n",
        "        target_W = self.embed.forward(idx)  # Get embeddings for given indices\n",
        "        out = np.sum(target_W * h, axis=1)  # Compute dot product between embeddings and input vector\n",
        "\n",
        "        self.cache = (h, target_W)  # Cache input vector and embeddings\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        # Backward pass method for EmbeddingDot layer\n",
        "        h, target_W = self.cache  # Retrieve cached data\n",
        "        dout = dout.reshape(dout.shape[0], 1)  # Reshape gradient\n",
        "\n",
        "        dtarget_W = dout * h  # Compute gradient for embeddings\n",
        "        self.embed.backward(dtarget_W)  # Backpropagate gradient through Embedding layer\n",
        "        dh = dout * target_W  # Compute gradient for input vector\n",
        "        return dh  # Return gradient for input vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGjg0hkwmCGw",
        "outputId": "a65960c4-6ddf-41cf-e46b-a3cab1d19fad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import numpy as np  # Import NumPy library\n",
        "\n",
        "np.random.choice(10)  # Randomly selects an integer from 0 to 9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q8Qern1mEqH",
        "outputId": "8547639b-5b63-48ff-970e-93f18501b003"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "np.random.choice(10) # Randomly selects an integer from 0 to 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YdPu6OXamIdL",
        "outputId": "3f5a620c-ee5e-4c32-a88f-aa4ae66e8d4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "words = ['you', 'say', 'goodbye', 'i', 'hello', '.']\n",
        "np.random.choice(words)  # Randomly selects a word from the list 'words'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXlvV6xZmMWj",
        "outputId": "1432b056-e8b2-4a82-cf92-35ec3d700989"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['goodbye', 'hello', 'say', 'you', 'i'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "np.random.choice(words, size=5, replace=False)\n",
        "# Randomly selects 5 unique words from the list 'words' without replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRntWsfvmUG9",
        "outputId": "e2781a4a-89ca-48a3-87d6-9372c0ffb15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.64196878 0.33150408 0.02652714]\n"
          ]
        }
      ],
      "source": [
        "p = [0.7, 0.29, 0.01]  # Probability distribution\n",
        "new_p = np.power(p, 0.75)  # Apply temperature scaling with factor 0.75\n",
        "\n",
        "new_p /= np.sum(new_p)  # Normalize probabilities to ensure they sum up to 1\n",
        "print(new_p)  # Print the normalized probabilities after temperature scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RofjvDeLmgP4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')  # Adds parent directory to the system path\n",
        "from common.np import *  # Import NumPy functions as np\n",
        "from common.layers import Embedding, SigmoidWithLoss  # Import custom layers\n",
        "import collections  # Import the collections module\n",
        "\n",
        "# Implementation of the Negative Sampling class\n",
        "# Located in chap04/negative_sampling_layer.py\n",
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        # Constructor method for UnigramSampler\n",
        "        self.sample_size = sample_size  # Number of negative samples to generate\n",
        "        self.vocab_size = None  # Vocabulary size\n",
        "        self.word_p = None  # Unigram distribution probabilities\n",
        "\n",
        "        # Count occurrences of each word in the corpus\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)  # Compute vocabulary size\n",
        "        self.vocab_size = vocab_size  # Store vocabulary size\n",
        "\n",
        "        # Calculate unigram probabilities using power transformation\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)  # Apply power transformation\n",
        "        self.word_p /= np.sum(self.word_p)  # Normalize probabilities to sum up to 1\n",
        "\n",
        "    def get_negative_sample(self, target):\n",
        "        # Method to generate negative samples\n",
        "        batch_size = target.shape[0]  # Get batch size\n",
        "\n",
        "        if not GPU:  # If running on CPU\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()  # Make a copy of the unigram distribution\n",
        "                target_idx = target[i]  # Get the target word index\n",
        "                p[target_idx] = 0  # Exclude target word from negative sampling\n",
        "                p /= p.sum()  # Normalize probabilities\n",
        "\n",
        "                # Generate negative samples without replacement based on probabilities\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size,\n",
        "                                                         size=self.sample_size,\n",
        "                                                         replace=False, p=p)\n",
        "        else:  # If running on GPU\n",
        "            # Generate negative samples with replacement based on probabilities\n",
        "            negative_sample = np.random.choice(self.vocab_size,\n",
        "                                               size=(batch_size, self.sample_size),\n",
        "                                               replace=True, p=self.word_p)\n",
        "\n",
        "        return negative_sample  # Return generated negative samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uglcKyl5mrKm",
        "outputId": "ec8ff968-57ea-4456-d2d4-eea58609b058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3 4]\n",
            " [2 0]\n",
            " [4 3]]\n"
          ]
        }
      ],
      "source": [
        "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])  # Example corpus\n",
        "power = 0.75  # Power parameter for unigram distribution\n",
        "sample_size = 2  # Number of negative samples to generate\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size)  # Initialize UnigramSampler\n",
        "target = np.array([1, 3, 0])  # Example target words\n",
        "negative_sample = sampler.get_negative_sample(target)  # Generate negative samples\n",
        "print(negative_sample)  # Print the generated negative samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zIjg1-Bhm4jx"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')  # Add parent directory to the system path\n",
        "from common.np import *  # Import NumPy functions as np\n",
        "from common.layers import Embedding, SigmoidWithLoss  # Import custom layers\n",
        "import collections  # Import the collections module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "FcEE-JJknmze"
      },
      "outputs": [],
      "source": [
        "# chap04/negative_sampling_layer.py\n",
        "class NegativeSamplingLoss:\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        # Constructor method for NegativeSamplingLoss\n",
        "        self.sample_size = sample_size  # Number of negative samples to generate\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)  # Initialize UnigramSampler\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]  # Initialize loss layers\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]  # Initialize EmbeddingDot layers\n",
        "\n",
        "        # Collect parameters and gradients from EmbeddingDot layers\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, h, target):\n",
        "        # Forward pass method for NegativeSamplingLoss\n",
        "        batch_size = target.shape[0]  # Get batch size\n",
        "        negative_sample = self.sampler.get_negative_sample(target)  # Generate negative samples\n",
        "\n",
        "        # Compute loss for positive samples\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        # Compute loss for negative samples\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]  # Get negative sample targets\n",
        "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        # Backward pass method for NegativeSamplingLoss\n",
        "        dh = 0\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "\n",
        "        return dh  # Return gradients with respect to input vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "pIwAGNStoGwG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')  # Adds parent directory to the system path\n",
        "from common.np import *  # Import NumPy functions as np\n",
        "from common.layers import Embedding  # Import the Embedding layer\n",
        "from negative_sampling_layer import NegativeSamplingLoss  # Import the NegativeSamplingLoss class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "t7CocKjWpEUJ"
      },
      "outputs": [],
      "source": [
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        # Constructor method for CBOW model\n",
        "        V, H = vocab_size, hidden_size  # Vocabulary size, hidden size\n",
        "\n",
        "        # Initialize input and output weight matrices\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')  # Input weight matrix\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype('f')  # Output weight matrix\n",
        "\n",
        "        # Initialize Embedding layers for input context words\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = Embedding(W_in)  # Embedding layer for context words\n",
        "            self.in_layers.append(layer)\n",
        "\n",
        "        # Initialize Negative Sampling Loss layer\n",
        "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        # Collect parameters and gradients from all layers\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # Store input and output word vectors\n",
        "        self.word_vecs1 = W_in  # Input word vectors\n",
        "        self.word_vecs2 = W_out  # Output word vectors\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        # Forward pass method for CBOW model\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])  # Sum up embeddings of context words\n",
        "        h *= 1 / len(self.in_layers)  # Average the embeddings\n",
        "        loss = self.ns_loss.forward(h, target)  # Compute loss using Negative Sampling Loss\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        # Backward pass method for CBOW model\n",
        "        dout = self.ns_loss.backward(dout)  # Backpropagate through Negative Sampling Loss\n",
        "        dout *= 1 / len(self.in_layers)  # Scale gradients\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout)  # Backpropagate through Embedding layers\n",
        "        return None  # No gradients with respect to input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3japeTglpUPe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')  # Adds parent directory to the system path\n",
        "import numpy as np  # Import NumPy library\n",
        "from common import config  # Import configuration settings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LTNuCytlpcGy"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pickle  # Import the pickle module for serialization\n",
        "from common.trainer import Trainer  # Import the Trainer class for model training\n",
        "from common.optimizer import Adam  # Import the Adam optimizer\n",
        "from cbow import CBOW  # Import the CBOW model\n",
        "from skip_gram import SkipGram  # Import the SkipGram model\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu  # Import utility functions\n",
        "from dataset import ptb  # Import the Penn Treebank dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "JHNX37whsMZc"
      },
      "outputs": [],
      "source": [
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        # Constructor method for CBOW model\n",
        "        V, H = vocab_size, hidden_size  # Vocabulary size, hidden size\n",
        "\n",
        "        # Initialize input and output weight matrices\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')  # Input weight matrix\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype('f')  # Output weight matrix\n",
        "\n",
        "        # Initialize Embedding layers for input context words\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = Embedding(W_in)  # Embedding layer for context words\n",
        "            self.in_layers.append(layer)\n",
        "\n",
        "        # Initialize Negative Sampling Loss layer\n",
        "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        # Collect parameters and gradients from all layers\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # Store input and output word vectors\n",
        "        self.word_vecs1 = W_in  # Input word vectors\n",
        "        self.word_vecs2 = W_out  # Output word vectors\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        # Forward pass method for CBOW model\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])  # Sum up embeddings of context words\n",
        "        h *= 1 / len(self.in_layers)  # Average the embeddings\n",
        "        loss = self.ns_loss.forward(h, target)  # Compute loss using Negative Sampling Loss\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        # Backward pass method for CBOW model\n",
        "        dout = self.ns_loss.backward(dout)  # Backpropagate through Negative Sampling Loss\n",
        "        dout *= 1 / len(self.in_layers)  # Scale gradients\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout)  # Backpropagate through Embedding layers\n",
        "        return None  # No gradients with respect to input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "OweQYQyEtUGT"
      },
      "outputs": [],
      "source": [
        "# chap04/train.py\n",
        "import sys\n",
        "sys.path.append('..')  # Add parent directory to the system path\n",
        "import numpy as np  # Import NumPy library\n",
        "from common import config  # Import configuration settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "tFvzH8XKtae7"
      },
      "outputs": [],
      "source": [
        "import pickle  # Import the pickle module for serialization\n",
        "from common.trainer import Trainer  # Import the Trainer class for model training\n",
        "from common.optimizer import Adam  # Import the Adam optimizer\n",
        "from cbow import CBOW  # Import the CBOW model\n",
        "from skip_gram import SkipGram  # Import the SkipGram model\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu  # Import utility functions\n",
        "from dataset import ptb  # Import the Penn Treebank dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "nUpjmJu7thcu"
      },
      "outputs": [],
      "source": [
        "window_size = 5  # Size of the context window\n",
        "hidden_size = 100  # Dimensionality of word embeddings and hidden layers\n",
        "batch_size = 100  # Number of samples per training batch\n",
        "max_epoch = 10  # Maximum number of training epochs\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')  # Load training data from the Penn Treebank dataset\n",
        "vocab_size = len(word_to_id)  # Vocabulary size\n",
        "\n",
        "# Create context-target pairs from the training corpus\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "\n",
        "# Move data to GPU if configured to use GPU\n",
        "if config.GPU:\n",
        "    contexts, target = to_gpu(contexts), to_gpu(target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "YoBmr_4MuMmc"
      },
      "outputs": [],
      "source": [
        "model = SkipGram(vocab_size, hidden_size, window_size, corpus)  # Initialize SkipGram model\n",
        "optimizer = Adam()  # Initialize Adam optimizer\n",
        "trainer = Trainer(model, optimizer)  # Initialize Trainer with model and optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z59myWdxuPYc"
      },
      "outputs": [],
      "source": [
        "#THIS SECTION TAKES TOO LONG TO RUN AND PRODUCE OUTPUT\n",
        "trainer.fit(contexts, target, max_epoch, batch_size, eval_interval=2000)  # Train the model\n",
        "trainer.plot()  # Plot training progress\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')  # Add parent directory to the system path\n",
        "import pickle  # Import the pickle module for serialization\n",
        "from common.util import most_similar, analogy  # Import utility functions for word similarity and analogy tasks\n"
      ],
      "metadata": {
        "id": "WQJoRcb1Kahi"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pkl_file = './cbow_params.pkl'  # Path to the pickle file containing CBOW model parameters\n",
        "with open(pkl_file, 'rb') as f:\n",
        "    params = pickle.load(f)  # Load parameters from the pickle file\n"
      ],
      "metadata": {
        "id": "bPEiSCgNLvVE"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vecs = params['word_vecs']  # Extract word vectors from loaded parameters\n",
        "word_to_id = params['word_to_id']  # Extract word-to-id dictionary from loaded parameters\n",
        "id_to_word = params['id_to_word']  # Extract id-to-word dictionary from loaded parameters\n"
      ],
      "metadata": {
        "id": "IF9zm1PnMcKr"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "querys = ['you', 'year', 'car', 'toyota']  # List of query words for similarity search\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)  # Find most similar words to each query\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uquNLMYbMhIL",
        "outputId": "253a26c5-3d83-4ac2-f464-bef5be56f871"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.6103515625\n",
            " someone: 0.59130859375\n",
            " i: 0.55419921875\n",
            " something: 0.48974609375\n",
            " anyone: 0.47314453125\n",
            "\n",
            "[query] year\n",
            " month: 0.71875\n",
            " week: 0.65234375\n",
            " spring: 0.62744140625\n",
            " summer: 0.6259765625\n",
            " decade: 0.603515625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.497314453125\n",
            " arabia: 0.47802734375\n",
            " auto: 0.47119140625\n",
            " disk-drive: 0.450927734375\n",
            " travel: 0.4091796875\n",
            "\n",
            "[query] toyota\n",
            " ford: 0.55078125\n",
            " instrumentation: 0.509765625\n",
            " mazda: 0.49365234375\n",
            " bethlehem: 0.47509765625\n",
            " nissan: 0.474853515625\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMyRfeWMM8XuglNMMymf0J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}